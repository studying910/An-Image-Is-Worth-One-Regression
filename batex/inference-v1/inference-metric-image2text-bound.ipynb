{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab4c2ab-a352-4299-9b2d-8784c0faaf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 25962 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 25962 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 16616 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eff3552d-f59d-4555-8bf1-e5b959b561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CLIP-space cosine-similarity distance\n",
    "import torch\n",
    "import os\n",
    "import clip\n",
    "import pathlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import sklearn.preprocessing\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from packaging import version\n",
    "from shutil import rmtree\n",
    "\n",
    "SEED = 1024\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "pretrained_model_name_or_path = \"/root/autodl-tmp/stable_diffusion/stable-diffusion-v1-5\"\n",
    "embedding_type = \"object\"\n",
    "trained_token = \"tortoise\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path,\n",
    "                                               torch_dtype=torch.float16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ffd9068-ef5a-4ff6-b461-31ba40a28fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding_type == \"object\":\n",
    "    prompt_list = [\n",
    "        # background modifications\n",
    "        \"a photo of {}\".format(trained_token),\n",
    "        \"a photo of {} on the beach\".format(trained_token),\n",
    "        \"a photo of {} on the moon\".format(trained_token),\n",
    "        \"a photo of {} on the table\".format(trained_token),\n",
    "    ]\n",
    "elif embedding_type == \"style\": \n",
    "    prompt_list = [\n",
    "        # object changes\n",
    "        \"a cat in the style of {}\".format(trained_token),\n",
    "        \"an apple in the style of {}\".format(trained_token),\n",
    "        \"a church in the style of {}\".format(trained_token),\n",
    "        \"a waterfall in the style of {}\".format(trained_token),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\"Embedding type should be either 'object' or 'style'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a39f2564-db4d-453e-9087-cf83cb6b8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        image = Image.open(c_data)\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class CLIPCapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        c_data = clip.tokenize(c_data, truncate=True).squeeze()\n",
    "        return {'caption': c_data}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1385f3d4-6ef0-47d0-84b8-da25746a7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_images(images, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPImageDataset(images),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_image_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['image'].to(device)\n",
    "            b = b.to(torch.float16)\n",
    "            all_image_features.append(model.encode_image(b).cpu().numpy())\n",
    "    all_image_features = np.vstack(all_image_features)\n",
    "    return all_image_features\n",
    "\n",
    "def extract_all_captions(captions, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPCapDataset(captions),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_text_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['caption'].to(device)\n",
    "            all_text_features.append(model.encode_text(b).cpu().numpy())\n",
    "    all_text_features = np.vstack(all_text_features)\n",
    "    return all_text_features\n",
    "\n",
    "def get_clip_score(model, clip_images, text_features, device, w=2.5):\n",
    "    if isinstance(clip_images, list):\n",
    "        # need to extract image features\n",
    "        clip_images = extract_all_images(clip_images, model, device)\n",
    "    \n",
    "    # as of numpy 1.21, normalize doesn't work properly for float16\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        clip_images = sklearn.preprocessing.normalize(clip_images, axis=1)\n",
    "        original_images = sklearn.preprocessing.normalize(original_images, axis=1)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'due to a numerical instability, new numpy normalization is slightly different than' \n",
    "            'paper results. To exactly replicate paper results, please use numpy version less' \n",
    "            'than 1.21, e.g., 1.20.3.')\n",
    "        clip_images = clip_images / np.sqrt(np.sum(clip_images ** 2, axis=1, keepdims=True))\n",
    "        text_features = text_features / np.sqrt(np.sum(text_features ** 2, axis=1, keepdims=True))\n",
    "    \n",
    "    per = w * np.clip(np.sum(clip_images * text_features, axis=1), 0, None)\n",
    "    return np.mean(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487471a-6bce-4bf1-8a2c-f60f4d6528f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute clip score for 1/4 prompt\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.07it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.58it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.92it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.00it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.89it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.02it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.03it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.09it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.96it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.04it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.90it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.80it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.63it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.80it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.66it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.62it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.34it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.39it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.34it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.59it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.85it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.86it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.94it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.98it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.84it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.70it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.75it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.84it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.95it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.82it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.95it/s]\n",
      " 42%|████▏     | 21/50 [00:01<00:01, 17.73it/s]"
     ]
    }
   ],
   "source": [
    "# calculate clip score for each prompt and average over it\n",
    "generator = None if SEED is None else torch.Generator(\n",
    "            device=DEVICE).manual_seed(SEED)\n",
    "N = 64  # number of random generated images\n",
    "temp_path = \"/root/autodl-tmp/textual_inversion/temp\"\n",
    "clip_image_dir = os.path.join(temp_path, \"clip_images_temp\")\n",
    "total_score = 0\n",
    "\n",
    "clip_model, clip_transform = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "for i, prompt in enumerate(prompt_list):\n",
    "    os.makedirs(clip_image_dir, exist_ok=True)\n",
    "    print(f\"compute clip score for {i + 1}/{len(prompt_list)} prompt\", end=\"\\r\")\n",
    "    \n",
    "    for n in range(N):\n",
    "        image_n = pipe(prompt, num_inference_steps=50, guidance_scale=7.5, \n",
    "                       generator=generator).images[0]\n",
    "        image_n_path = os.path.join(clip_image_dir, \"{}_{}.png\".format(prompt, n + 1))\n",
    "        image_n.save(image_n_path)\n",
    "    \n",
    "    clip_images_path_list = [os.path.join(clip_image_dir, path) for path in os.listdir(\n",
    "                             clip_image_dir) if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]  \n",
    "    clip_features = extract_all_images(clip_images_path_list, clip_model, DEVICE, batch_size=N, \n",
    "                                       num_workers=8)\n",
    "    \n",
    "    # get text features\n",
    "    text_candidates = [prompt] * N # .replace(trained_token, initialization_word)\n",
    "    text_features = extract_all_captions(text_candidates, clip_model, DEVICE, batch_size=N, num_workers=8)\n",
    "    \n",
    "    # compute Clip-space cosine similarity\n",
    "    once_score = get_clip_score(clip_model, clip_features, text_features, DEVICE)\n",
    "    total_score += once_score\n",
    "    \n",
    "    # empty the clip_image_dir\n",
    "    rmtree(clip_image_dir)\n",
    "\n",
    "# compute and save the final score\n",
    "final_score = total_score / len(prompt_list)\n",
    "print(\"END!!! CLIP image2text score is: {}\".format(final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7657413-2486-4f12-a026-7a6a553b846c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dushian",
   "language": "python",
   "name": "dushian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
