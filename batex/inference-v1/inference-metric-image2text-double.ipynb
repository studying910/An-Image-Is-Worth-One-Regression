{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551cc1b2-0915-49aa-9269-8c167298abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 28478 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 28478 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 19578 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dbaa9e-0e8e-4755-b44c-18de94ae009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CLIP-space cosine-similarity distance\n",
    "import torch\n",
    "import os\n",
    "import clip\n",
    "import pathlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import sklearn.preprocessing\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from packaging import version\n",
    "from shutil import rmtree\n",
    "\n",
    "SEED = 512\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "pretrained_model_name_or_path = \"/root/autodl-tmp/stable_diffusion/stable-diffusion-v1-5\"\n",
    "learned_embeds_path = \"/root/autodl-tmp/textual_inversion/merged_embeddings/custom_chair&custom_cat/learned_embeds_factor=0.12&learned_embeds_factor=0.5.bin\"  \n",
    "all_embedding_path, embeds_suffix = os.path.split(learned_embeds_path)\n",
    "embeds_name, _ = os.path.splitext(embeds_suffix)\n",
    "_, all_dataset_name = os.path.split(all_embedding_path)\n",
    "dataset_name_list = all_dataset_name.split(\"&\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc15cc11-9e3a-40bd-a58e-f9476f7c16b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder token for dataset custom_chair: <custom_chair>\n",
      "placeholder token for dataset custom_cat: <custom_cat>\n"
     ]
    }
   ],
   "source": [
    "loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
    "trained_token_list = list(loaded_learned_embeds.keys())\n",
    "dtype = text_encoder.get_input_embeddings().weight.dtype\n",
    "for i, trained_token in enumerate(trained_token_list):\n",
    "    # separate token and the embeds\n",
    "    embeds = loaded_learned_embeds[trained_token]\n",
    "    # cast to dtype of text_encoder\n",
    "    embeds.to(dtype)\n",
    "    # add the token in tokenizer\n",
    "    num_added_tokens = tokenizer.add_tokens(trained_token)\n",
    "    # resize the token embeddings\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "    # get the id for the token and assign the embeds\n",
    "    token_id = tokenizer.convert_tokens_to_ids(trained_token)\n",
    "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(f\"The tokenizer already contains the token {trained_token}. \"\n",
    "                         \"Please pass a different `token` that is not already in the tokenizer.\")\n",
    "    print(f\"placeholder token for dataset {dataset_name_list[i]}: {trained_token}\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path,\n",
    "                                               torch_dtype=torch.float16, \n",
    "                                               text_encoder=text_encoder, \n",
    "                                               tokenizer=tokenizer).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f4b491a-f2e6-47a0-9919-9f8cda192751",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    # object&object\n",
    "    \"a photo of {} and {}\".format(trained_token_list[0], trained_token_list[1]),\n",
    "    \"a photo of {} and {} on the beach\".format(trained_token_list[0], trained_token_list[1]),\n",
    "    \"a photo of {} and {} on the moon\".format(trained_token_list[0], trained_token_list[1]),\n",
    "    \"a photo of {} and {} in Times Square\".format(trained_token_list[0], trained_token_list[1]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5eb72b-a6b6-4836-8e72-54ff647427bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        image = Image.open(c_data)\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "class CLIPCapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        c_data = clip.tokenize(c_data, truncate=True).squeeze()\n",
    "        return {'caption': c_data}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd4e8790-07db-4e12-8b11-6a65873132d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_images(images, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPImageDataset(images),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_image_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['image'].to(device)\n",
    "            b = b.to(torch.float16)\n",
    "            all_image_features.append(model.encode_image(b).cpu().numpy())\n",
    "    all_image_features = np.vstack(all_image_features)\n",
    "    return all_image_features\n",
    "\n",
    "def extract_all_captions(captions, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPCapDataset(captions),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_text_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['caption'].to(device)\n",
    "            all_text_features.append(model.encode_text(b).cpu().numpy())\n",
    "    all_text_features = np.vstack(all_text_features)\n",
    "    return all_text_features\n",
    "\n",
    "def get_clip_score(model, clip_images, text_features, device, w=2.5):\n",
    "    if isinstance(clip_images, list):\n",
    "        # need to extract image features\n",
    "        clip_images = extract_all_images(clip_images, model, device)\n",
    "    \n",
    "    # as of numpy 1.21, normalize doesn't work properly for float16\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        clip_images = sklearn.preprocessing.normalize(clip_images, axis=1)\n",
    "        original_images = sklearn.preprocessing.normalize(original_images, axis=1)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'due to a numerical instability, new numpy normalization is slightly different than' \n",
    "            'paper results. To exactly replicate paper results, please use numpy version less' \n",
    "            'than 1.21, e.g., 1.20.3.')\n",
    "        clip_images = clip_images / np.sqrt(np.sum(clip_images ** 2, axis=1, keepdims=True))\n",
    "        text_features = text_features / np.sqrt(np.sum(text_features ** 2, axis=1, keepdims=True))\n",
    "    \n",
    "    per = w * np.clip(np.sum(clip_images * text_features, axis=1), 0, None)\n",
    "    return np.mean(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2bef6eb-2330-42c0-9762-3b49b32063a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute clip score for 1/4 prompt\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.37it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.61it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.86it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.90it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.42it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.47it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.37it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.36it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.36it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.26it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.30it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.34it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.37it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.42it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.32it/s]\n",
      "/tmp/ipykernel_254831/1222842887.py:9: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  Resize(n_px, interpolation=Image.BICUBIC),\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "/tmp/ipykernel_254831/4166279868.py:36: UserWarning: due to a numerical instability, new numpy normalization is slightly different thanpaper results. To exactly replicate paper results, please use numpy version lessthan 1.21, e.g., 1.20.3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute clip score for 2/4 prompt\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.12it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.25it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.26it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.26it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.23it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.33it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.15it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.17it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.16it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.14it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.28it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute clip score for 3/4 prompt\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.27it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END!!! CLIP image2text score for learned_embeds_factor=0.12&learned_embeds_factor=0.5 is: 0.6785888671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate clip score for each prompt and average over it\n",
    "generator = None if SEED is None else torch.Generator(\n",
    "            device=DEVICE).manual_seed(SEED)\n",
    "N = 16  # number of random generated images\n",
    "clip_image_dir = os.path.join(all_embedding_path, \"clip_images_temp\")\n",
    "total_score = 0\n",
    "\n",
    "clip_model, clip_transform = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "for i, prompt in enumerate(prompt_list):\n",
    "    os.makedirs(clip_image_dir, exist_ok=True)\n",
    "    print(f\"compute clip score for {i + 1}/{len(prompt_list)} prompt\", end=\"\\r\")\n",
    "    \n",
    "    for n in range(N):\n",
    "        image_n = pipe(prompt, num_inference_steps=50, guidance_scale=7.5, \n",
    "                       generator=generator).images[0]\n",
    "        image_n_path = os.path.join(clip_image_dir, \"{}_{}.png\".format(prompt, n + 1))\n",
    "        image_n.save(image_n_path)\n",
    "    \n",
    "    clip_images_path_list = [os.path.join(clip_image_dir, path) for path in os.listdir(\n",
    "                             clip_image_dir) if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]  \n",
    "    clip_features = extract_all_images(clip_images_path_list, clip_model, DEVICE, batch_size=N, \n",
    "                                   num_workers=8)\n",
    "    \n",
    "    # get text features\n",
    "    text_candidates = [prompt] * N # .replace(trained_token, initialization_word)\n",
    "    text_features = extract_all_captions(text_candidates, clip_model, DEVICE, batch_size=N, num_workers=8)\n",
    "    \n",
    "    # compute Clip-space cosine similarity\n",
    "    once_score = get_clip_score(clip_model, clip_features, text_features, DEVICE)\n",
    "    total_score += once_score\n",
    "    \n",
    "    # empty the clip_image_dir\n",
    "    rmtree(clip_image_dir)\n",
    "\n",
    "# compute and save the final score\n",
    "final_score = total_score / len(prompt_list)\n",
    "clip_score_dir = f\"{all_embedding_path}/i2t_score\"\n",
    "os.makedirs(clip_score_dir, exist_ok=True)\n",
    "clip_score_path = f\"{clip_score_dir}/{embeds_name}_i2t_score.txt\"\n",
    "with open(clip_score_path, \"w\") as f:\n",
    "    f.write(\"CLIP image2text score: {}\".format(final_score))\n",
    "print(\"END!!! CLIP image2text score for {} is: {}\".format(embeds_name, final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95dfc7-e9c0-4904-ae94-03a263a4911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dushian",
   "language": "python",
   "name": "dushian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
