{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ffc67a-ea63-435e-a710-fe62f3dd96dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 19652 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 28478 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 19578 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd53506-abbe-49cd-93fe-3661faac23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CLIP-space cosine-similarity distance\n",
    "import torch\n",
    "import os\n",
    "import clip\n",
    "import pathlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import sklearn.preprocessing\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from packaging import version\n",
    "from shutil import rmtree\n",
    "\n",
    "SEED = None\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "pretrained_model_name_or_path = \"/root/autodl-tmp/stable_diffusion/stable-diffusion-v1-5\"\n",
    "learned_embeds_path = \"/root/autodl-tmp/textual_inversion/merged_embeddings/custom_chair&custom_cat/learned_embeds_factor=0.12&learned_embeds_factor=0.5.bin\"\n",
    "original_image_dir_1 = \"/root/autodl-tmp/textual_inversion/data/chair\"\n",
    "original_image_dir_2 = \"/root/autodl-tmp/textual_inversion/data/cat\"\n",
    "all_embedding_path, embeds_suffix = os.path.split(learned_embeds_path)\n",
    "embeds_name, _ = os.path.splitext(embeds_suffix)\n",
    "_, all_dataset_name = os.path.split(all_embedding_path)\n",
    "dataset_name_list = all_dataset_name.split(\"&\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51bd969b-2d53-4387-afb4-44610670280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder token for dataset custom_chair: <custom_chair>\n",
      "placeholder token for dataset custom_cat: <custom_cat>\n"
     ]
    }
   ],
   "source": [
    "loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
    "trained_token_list = list(loaded_learned_embeds.keys())\n",
    "dtype = text_encoder.get_input_embeddings().weight.dtype\n",
    "for i, trained_token in enumerate(trained_token_list):\n",
    "    # separate token and the embeds\n",
    "    embeds = loaded_learned_embeds[trained_token]\n",
    "    # cast to dtype of text_encoder\n",
    "    embeds.to(dtype)\n",
    "    # add the token in tokenizer\n",
    "    num_added_tokens = tokenizer.add_tokens(trained_token)\n",
    "    # resize the token embeddings\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "    # get the id for the token and assign the embeds\n",
    "    token_id = tokenizer.convert_tokens_to_ids(trained_token)\n",
    "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(f\"The tokenizer already contains the token {trained_token}. \"\n",
    "                         \"Please pass a different `token` that is not already in the tokenizer.\")\n",
    "    print(f\"placeholder token for dataset {dataset_name_list[i]}: {trained_token}\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path,\n",
    "                                               torch_dtype=torch.float16, \n",
    "                                               text_encoder=text_encoder, \n",
    "                                               tokenizer=tokenizer).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c66f86-e8cc-4593-93a8-2c8eec2785c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 1/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 2/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 3/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 4/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 5/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 6/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 7/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 8/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 9/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 10/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 11/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 12/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 13/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 14/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 15/16\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images: 16/16\r"
     ]
    }
   ],
   "source": [
    "prompt = \"A photo of {} and {}\".format(trained_token_list[0], trained_token_list[1]) # Or \"A photo depicts <*>\"\n",
    "generator = None if SEED is None else torch.Generator(\n",
    "            device=DEVICE).manual_seed(SEED)\n",
    "N = 16  # number of random generated images\n",
    "clip_image_dir = os.path.join(all_embedding_path, \"clip_images\")\n",
    "os.makedirs(clip_image_dir, exist_ok=True)\n",
    "\n",
    "for n in range(N):\n",
    "    image_n = pipe(prompt, num_inference_steps=50, guidance_scale=7.5, \n",
    "                   generator=generator).images[0]\n",
    "    print(f\"Generating images: {n + 1}/{N}\", end=\"\\r\")\n",
    "    image_n_path = os.path.join(clip_image_dir, \"{}_{}.png\".format(prompt, n + 1))\n",
    "    image_n.save(image_n_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79c81b49-9777-498a-af4b-b75a9a238eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_images_path_list = [os.path.join(clip_image_dir, path) for path in os.listdir(clip_image_dir)\n",
    "                         if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]\n",
    "original_images_path_list_1 = [os.path.join(original_image_dir_1, path) for path in os.listdir(\n",
    "                        original_image_dir_1) if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]\n",
    "num_original_images_1 = len(original_images_path_list_1)\n",
    "original_images_path_list_2 = [os.path.join(original_image_dir_2, path) for path in os.listdir(\n",
    "                        original_image_dir_2) if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]\n",
    "num_original_images_2 = len(original_images_path_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0badac79-beb5-40e7-8b6b-97a0a33f486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        image = Image.open(c_data)\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae928b42-05e2-495b-9731-a14e04c039ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_images(images, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPImageDataset(images),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_image_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['image'].to(device)\n",
    "            b = b.to(torch.float16)\n",
    "            all_image_features.append(model.encode_image(b).cpu().numpy())\n",
    "    all_image_features = np.vstack(all_image_features)\n",
    "    return all_image_features\n",
    "\n",
    "def get_clip_score(model, clip_images, original_images, device, w=1.0):\n",
    "    if isinstance(clip_images, list):\n",
    "        # need to extract image features\n",
    "        clip_images = extract_all_images(clip_images, model, device)\n",
    "    if isinstance(original_images, list):\n",
    "        # need to extract image features\n",
    "        original_images = extract_all_images(original_images, model, device)\n",
    "    \n",
    "    # as of numpy 1.21, normalize doesn't work properly for float16\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        clip_images = sklearn.preprocessing.normalize(clip_images, axis=1)\n",
    "        original_images = sklearn.preprocessing.normalize(original_images, axis=1)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'due to a numerical instability, new numpy normalization is slightly different than' \n",
    "            'paper results. To exactly replicate paper results, please use numpy version less' \n",
    "            'than 1.21, e.g., 1.20.3.')\n",
    "        clip_images = clip_images / np.sqrt(np.sum(clip_images ** 2, axis=1, keepdims=True))\n",
    "        original_images = original_images / np.sqrt(np.sum(original_images ** 2, axis=1, \n",
    "                                                           keepdims=True))\n",
    "    \n",
    "    per = w * np.clip(np.dot(clip_images, original_images.T), 0, None)\n",
    "    return np.mean(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0930fe9c-1ffc-43f9-91d8-8c2006b3f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268302/2880399981.py:9: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  Resize(n_px, interpolation=Image.BICUBIC),\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END!!! CLIP image2image score for learned_embeds_factor=0.12&learned_embeds_factor=0.5 is: 0.51806640625&0.65576171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_268302/2946529958.py:27: UserWarning: due to a numerical instability, new numpy normalization is slightly different thanpaper results. To exactly replicate paper results, please use numpy version lessthan 1.21, e.g., 1.20.3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_transform = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "clip_features = extract_all_images(clip_images_path_list, clip_model, DEVICE, batch_size=N, \n",
    "                                   num_workers=8)\n",
    "original_features_1 = extract_all_images(original_images_path_list_1, clip_model, DEVICE, \n",
    "                                       batch_size=num_original_images_1, num_workers=8)\n",
    "original_features_2 = extract_all_images(original_images_path_list_2, clip_model, DEVICE, \n",
    "                                       batch_size=num_original_images_2, num_workers=8)\n",
    "\n",
    "# Compute pair-wise Clip-space cosine similarity\n",
    "final_score_1 = get_clip_score(clip_model, clip_features, original_features_1, DEVICE)\n",
    "final_score_2 = get_clip_score(clip_model, clip_features, original_features_2, DEVICE)\n",
    "clip_score_dir = f\"{all_embedding_path}/i2i_score\"\n",
    "os.makedirs(clip_score_dir, exist_ok=True)\n",
    "clip_score_path = f\"{clip_score_dir}/{embeds_name}_i2i_score.txt\"\n",
    "with open(clip_score_path,\"w\") as f:\n",
    "    f.write(\"CLIP image2image score: {}&{}\".format(final_score_1, final_score_2))\n",
    "print(\"END!!! CLIP image2image score for {} is: {}&{}\".format(embeds_name, final_score_1, final_score_2))\n",
    "rmtree(clip_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca72d2f-9495-4b5b-9920-2afe85860598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dushian",
   "language": "python",
   "name": "dushian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
