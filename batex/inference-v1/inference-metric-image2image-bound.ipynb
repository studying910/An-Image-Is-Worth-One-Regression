{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4846604e-4f68-4ed9-a744-bd660e993c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 16716 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 25962 MiB\n",
      "NVIDIA A100-PCIE-40GB, 40960 MiB, 16616 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be45c0d4-b7c5-4843-830e-7f9558e5a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CLIP-space cosine-similarity distance\n",
    "import torch\n",
    "import os\n",
    "import clip\n",
    "import pathlib\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import sklearn.preprocessing\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from packaging import version\n",
    "from shutil import rmtree\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "pretrained_model_name_or_path = \"/root/autodl-tmp/stable_diffusion/stable-diffusion-v1-5\"\n",
    "original_image_dir = \"/root/autodl-tmp/textual_inversion/download_data/cute-game-style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a1e6488-b130-4444-9079-21767420e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_images_path_list = [os.path.join(original_image_dir, path) for path in os.listdir(original_image_dir)\n",
    "                         if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]\n",
    "original_images_path_list = [os.path.join(original_image_dir, path) for path in os.listdir(\n",
    "                             original_image_dir) if path.endswith(('.png', '.jpg', '.jpeg', '.tiff'))]\n",
    "num_original_images = len(original_images_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3abd87f4-e211-4b49-afa2-bebf657942ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        image = Image.open(c_data)\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6e94a4e-4d31-48fd-8739-ce57911b6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_images(images, model, device, batch_size=64, num_workers=8):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPImageDataset(images),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_image_features = []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm.tqdm(data):\n",
    "            b = b['image'].to(device)\n",
    "            b = b.to(torch.float16)\n",
    "            all_image_features.append(model.encode_image(b).cpu().numpy())\n",
    "    all_image_features = np.vstack(all_image_features)\n",
    "    return all_image_features\n",
    "\n",
    "def get_clip_score(model, clip_images, original_images, device, w=1.0):\n",
    "    if isinstance(clip_images, list):\n",
    "        # need to extract image features\n",
    "        clip_images = extract_all_images(clip_images, model, device)\n",
    "    if isinstance(original_images, list):\n",
    "        # need to extract image features\n",
    "        original_images = extract_all_images(original_images, model, device)\n",
    "    \n",
    "    # as of numpy 1.21, normalize doesn't work properly for float16\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        clip_images = sklearn.preprocessing.normalize(clip_images, axis=1)\n",
    "        original_images = sklearn.preprocessing.normalize(original_images, axis=1)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'due to a numerical instability, new numpy normalization is slightly different than' \n",
    "            'paper results. To exactly replicate paper results, please use numpy version less' \n",
    "            'than 1.21, e.g., 1.20.3.')\n",
    "        clip_images = clip_images / np.sqrt(np.sum(clip_images ** 2, axis=1, keepdims=True))\n",
    "        original_images = original_images / np.sqrt(np.sum(original_images ** 2, axis=1, \n",
    "                                                           keepdims=True))\n",
    "    \n",
    "    per = w * np.clip(np.dot(clip_images, original_images.T), 0, None)\n",
    "    return np.mean(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0759a0d4-435d-4160-b8db-8aa9eaaec54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_708350/2880399981.py:9: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  Resize(n_px, interpolation=Image.BICUBIC),\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END!!! CLIP image2image score is: 0.69140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_708350/2946529958.py:27: UserWarning: due to a numerical instability, new numpy normalization is slightly different thanpaper results. To exactly replicate paper results, please use numpy version lessthan 1.21, e.g., 1.20.3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_transform = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "clip_features = extract_all_images(clip_images_path_list, clip_model, DEVICE, \n",
    "                                   batch_size=num_original_images, num_workers=8)\n",
    "original_features = extract_all_images(original_images_path_list, clip_model, DEVICE, \n",
    "                                       batch_size=num_original_images, num_workers=8)\n",
    "\n",
    "# Compute pair-wise Clip-space cosine similarity\n",
    "final_score = get_clip_score(clip_model, clip_features, original_features, DEVICE)\n",
    "print(\"END!!! CLIP image2image score is: {}\".format(final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b46d6-21f5-40a0-93ad-013e42b13cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dushian",
   "language": "python",
   "name": "dushian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
